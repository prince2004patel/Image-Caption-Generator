{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heJBIUEasziI",
        "outputId": "4d8281e0-1d87-47c2-e9ea-0a6f8b2e47a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQxGMnpsYpS0",
        "outputId": "d0ada02b-fb31-4235-b6b9-39a7fdf94277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available: 0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0vjIEHJYqcX",
        "outputId": "778a0d4b-42b9-4288-d65f-5b963ffecb8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-748f3db27496>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow using: 2.18.0\n",
            "Running on GPU: False\n"
          ]
        }
      ],
      "source": [
        "print(\"TensorFlow using:\", tf.__version__)\n",
        "print(\"Running on GPU:\", tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MEkyKV6ME6kn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tqdm\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VdqIW9aPF0eh"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/drive/MyDrive/ML/image_caption/data/Images'\n",
        "captions_file = '/content/drive/MyDrive/ML/image_caption/data/captions.txt'\n",
        "feature_save_path = '/content/drive/MyDrive/ML/image_caption/image_features.pkl'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP95Wb7STODA"
      },
      "source": [
        "# Clean the Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od45U5x6F3Zl",
        "outputId": "d2183d93-daff-4ba5-b2b2-f21586b1f35f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total valid image IDs: 8091\n"
          ]
        }
      ],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "def load_captions(file_path):\n",
        "    descriptions = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        next(f)  # skip header\n",
        "        for line in f:\n",
        "            tokens = line.strip().split(',')\n",
        "            if len(tokens) != 2:\n",
        "                continue\n",
        "            img_name, caption = tokens\n",
        "            img_id = img_name.split('.')[0]\n",
        "            caption = clean_text(caption)\n",
        "            if img_id not in descriptions:\n",
        "                descriptions[img_id] = []\n",
        "            descriptions[img_id].append(f\"startseq {caption} endseq\")\n",
        "    return descriptions\n",
        "\n",
        "descriptions = load_captions(captions_file)\n",
        "print(\"Total valid image IDs:\", len(descriptions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBeYUjt8TSVv"
      },
      "source": [
        "# Extract feaures using ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JyLKRGeF9Zb",
        "outputId": "696e6d78-3993-42d0-aead-88c40e31951e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total matching image IDs for feature extraction: 8091\n",
            "ResNet50 model loaded (top layer removed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8091/8091 [15:25<00:00,  8.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image features saved at: /content/drive/MyDrive/ML/image_caption/image_features.pkl\n",
            "Total features extracted: 8091\n"
          ]
        }
      ],
      "source": [
        "def extract_image_features(image_path, valid_img_ids):\n",
        "    resnet = ResNet50(weights='imagenet')\n",
        "    model = Model(inputs=resnet.input, outputs=resnet.layers[-2].output)\n",
        "    print(\"ResNet50 model loaded (top layer removed)\")\n",
        "\n",
        "    def preprocess_img(img_path):\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = preprocess_input(img_array)\n",
        "        return img_array\n",
        "\n",
        "    features = {}\n",
        "    for img_id in tqdm.tqdm(valid_img_ids):\n",
        "        full_path = os.path.join(image_path, img_id + '.jpg')\n",
        "        if not os.path.exists(full_path):\n",
        "            for ext in ['.jpeg', '.png']:\n",
        "                full_path = os.path.join(image_path, img_id + ext)\n",
        "                if os.path.exists(full_path):\n",
        "                    break\n",
        "            else:\n",
        "                print(\"Image not found for ID:\", img_id)\n",
        "                continue\n",
        "\n",
        "        img_array = preprocess_img(full_path)\n",
        "        feature_vector = model.predict(img_array, verbose=0)\n",
        "        features[img_id] = feature_vector.flatten()\n",
        "\n",
        "    return features\n",
        "\n",
        "# Get image IDs that actually exist in the folder\n",
        "available_images = set(os.path.splitext(f)[0] for f in os.listdir(image_path)\n",
        "                       if f.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
        "valid_img_ids = descriptions.keys() & available_images\n",
        "print(\"Total matching image IDs for feature extraction:\", len(valid_img_ids))\n",
        "\n",
        "# Extract and save features\n",
        "features = extract_image_features(image_path, valid_img_ids)\n",
        "\n",
        "with open(feature_save_path, 'wb') as f:\n",
        "    pickle.dump(features, f)\n",
        "\n",
        "print(\"Image features saved at:\", feature_save_path)\n",
        "print(\"Total features extracted:\", len(features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trqyw1wEThh-"
      },
      "source": [
        "# Tokenize Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4ndLThBGSmo",
        "outputId": "72b0d100-eaa6-49c7-adda-c843875c357b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sequences: 38008\n",
            "Vocabulary size: 8520\n",
            "Max sequence length: 35\n",
            "First 5 sequences: [[2, 1, 42, 4, 1, 91, 171, 7, 114, 51, 1, 396, 12, 377, 4, 28, 5001, 689, 3], [2, 1, 18, 303, 62, 1, 188, 115, 3], [2, 1, 37, 18, 114, 62, 1, 188, 2328, 3], [2, 1, 37, 18, 114, 5, 377, 20, 61, 2328, 3], [2, 1, 37, 18, 4, 1, 91, 171, 303, 62, 1, 188, 3266, 3]]\n"
          ]
        }
      ],
      "source": [
        "# flatten all captions into a list\n",
        "all_captions = [caption for captions in descriptions.values() for caption in captions]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "# convert captions to sequences of intergers\n",
        "sequences = tokenizer.texts_to_sequences(all_captions)\n",
        "\n",
        "# define maximum sequnce length\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# create the word-to-index dictionary\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Total sequences:\", len(sequences))\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "print(\"Max sequence length:\", max_sequence_length)\n",
        "print(\"First 5 sequences:\", sequences[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCvtkPzTmNz"
      },
      "source": [
        "# Prepare Input-Output Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJTICMa2a021",
        "outputId": "c2fa4710-2a23-41a8-edc2-79f3ab5bd86c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total loaded image features: 8091\n"
          ]
        }
      ],
      "source": [
        "# Load features\n",
        "with open(feature_save_path, 'rb') as f:\n",
        "    features = pickle.load(f)\n",
        "\n",
        "print(\"Total loaded image features:\", len(features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kd52hqRGf7L",
        "outputId": "e3114154-4b48-4f7c-c20d-dddfa96ddcb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1 (image features) shape: (441365, 2048)\n",
            "X2 (caption sequences) shape: (441365, 35)\n",
            "y (next word) shape: (441365,)\n"
          ]
        }
      ],
      "source": [
        "image_features = []\n",
        "input_sequences = []\n",
        "output_words = []\n",
        "\n",
        "for img_id, caption_list in descriptions.items():\n",
        "    if img_id not in features:\n",
        "        continue\n",
        "    image_feature = features[img_id]\n",
        "\n",
        "    for caption in caption_list:\n",
        "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "        for i in range(1, len(sequence)):\n",
        "            in_seq = sequence[:i]\n",
        "            out_word = sequence[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_sequence_length, padding='post')[0]\n",
        "\n",
        "            image_features.append(image_feature)\n",
        "            input_sequences.append(in_seq)\n",
        "            output_words.append(out_word)\n",
        "\n",
        "X1 = np.array(image_features)\n",
        "X2 = np.array(input_sequences)\n",
        "y = np.array(output_words)\n",
        "\n",
        "print(\"X1 (image features) shape:\", X1.shape)\n",
        "print(\"X2 (caption sequences) shape:\", X2.shape)\n",
        "print(\"y (next word) shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oRTZKrvIL_z"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/ML/image_caption/data/X1.npy', X1)\n",
        "np.save('/content/drive/MyDrive/ML/image_caption/data/X2.npy', X2)\n",
        "np.save('/content/drive/MyDrive/ML/image_caption/data/y.npy', y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeQAVhmIUBQd"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI8hIkoRINFw"
      },
      "outputs": [],
      "source": [
        "X1_load = np.load('/content/drive/MyDrive/ML/image_caption/data/X1.npy')\n",
        "X2_load = np.load('/content/drive/MyDrive/ML/image_caption/data/X2.npy')\n",
        "y_load = np.load('/content/drive/MyDrive/ML/image_caption/data/y.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "exdfdoEvKOOm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, add\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XafaajVKael"
      },
      "outputs": [],
      "source": [
        "# One-hot encode the output y\n",
        "# vocab_size = len(tokenizer.word_index) + 1  # already defined\n",
        "# vocab_size = 8520\n",
        "# y = to_categorical(y, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAuKYRaMKkJv",
        "outputId": "83e5bf3b-2e98-48ac-85cb-7fa594e0cca3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  1,  42,   4, ..., 114, 107,   3])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUn5VvSJKp5p"
      },
      "source": [
        "# Define the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMChyfiU2xmD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# image feature vector input (2048-dim)\n",
        "input1 = Input(shape=(2048,))\n",
        "img_dense = Dense(128, activation='relu')(input1)  # Reduced to 128\n",
        "img_dropout = Dropout(0.3)(img_dense)\n",
        "\n",
        "# caption sequence input\n",
        "max_sequence_length = 35\n",
        "vocab_size = 8520\n",
        "\n",
        "input2 = Input(shape=(max_sequence_length,))\n",
        "cap_embedding = Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)(input2)\n",
        "cap_dropout = Dropout(0.3)(cap_embedding)\n",
        "cap_lstm = LSTM(128)(cap_dropout)  # Reduced units\n",
        "\n",
        "# merge both models\n",
        "decoder = add([img_dropout, cap_lstm])\n",
        "decoder = Dense(128, activation='relu')(decoder)\n",
        "output = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# define model\n",
        "model = Model(inputs=[input1, input2], outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "aEp19COmLqcw",
        "outputId": "d28ffb20-8488-4f93-c5b5-a7f406544234"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,090,560</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8520</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,099,080</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,090,560\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m262,272\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m131,584\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8520\u001b[0m)      │  \u001b[38;5;34m1,099,080\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,600,008</span> (9.92 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,600,008\u001b[0m (9.92 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,600,008</span> (9.92 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,600,008\u001b[0m (9.92 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3pChtuDLyYB",
        "outputId": "fe0f6bb5-2b49-452e-8994-e21f67fbb5f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 212ms/step - accuracy: 0.2620 - loss: 4.4448\n",
            "Epoch 2/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 212ms/step - accuracy: 0.3587 - loss: 3.2750\n",
            "Epoch 3/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 213ms/step - accuracy: 0.3806 - loss: 3.0137\n",
            "Epoch 4/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 213ms/step - accuracy: 0.3908 - loss: 2.8628\n",
            "Epoch 5/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 213ms/step - accuracy: 0.3999 - loss: 2.7559\n",
            "Epoch 6/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m724s\u001b[0m 210ms/step - accuracy: 0.4071 - loss: 2.6866\n",
            "Epoch 7/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m748s\u001b[0m 211ms/step - accuracy: 0.4105 - loss: 2.6292\n",
            "Epoch 8/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 211ms/step - accuracy: 0.4150 - loss: 2.5772\n",
            "Epoch 9/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 210ms/step - accuracy: 0.4198 - loss: 2.5380\n",
            "Epoch 10/10\n",
            "\u001b[1m3449/3449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m714s\u001b[0m 207ms/step - accuracy: 0.4220 - loss: 2.5087\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ea99b9fe690>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit([X1_load, X2_load], y_load, epochs=10, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XXVknf7L8DV",
        "outputId": "38792ff3-2822-4254-8d14-84ec89dc09a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/MyDrive/ML/image_caption/caption_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ60ud-tVt-v"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/ML/image_caption/img_caption_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktXZUEsHJ9El"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/ML/image_caption/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6wHsNicJ_iI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "params = {\n",
        "    \"max_sequence_length\": max_sequence_length,\n",
        "    \"vocab_size\": vocab_size\n",
        "}\n",
        "\n",
        "with open('/content/drive/MyDrive/ML/image_caption/model_params.json', 'w') as f:\n",
        "    json.dump(params, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "582IxvvPViEr",
        "outputId": "992f9fea-facb-49f5-ba1f-69853a2ba528"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 12 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# Load the model without using custom_object_scope initially\n",
        "# model = load_model('/content/drive/MyDrive/ML/image_caption/caption_model.h5')\n",
        "model  = load_model('/content/drive/MyDrive/ML/image_caption/img_caption_model.keras')\n",
        "\n",
        "# Load tokenizer\n",
        "with open('/content/drive/MyDrive/ML/image_caption/tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# Load image features\n",
        "with open('/content/drive/MyDrive/ML/image_caption/image_features.pkl', 'rb') as f:\n",
        "    features = pickle.load(f)\n",
        "\n",
        "# Load config.json\n",
        "with open('/content/drive/MyDrive/ML/image_caption/model_params.json', 'r') as f:\n",
        "    model_params = json.load(f)\n",
        "\n",
        "# Extract parameters\n",
        "max_sequence_length = model_params['max_sequence_length']\n",
        "vocab_size = model_params['vocab_size']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CCisAdNxyUA",
        "outputId": "2c874236-9909-4d9d-be98-dfa4b04a68c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [32:48<00:00,  1.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Predicted captions saved for 1000 images: /content/drive/MyDrive/ML/image_caption/predicted_captions_1000.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the caption generator\n",
        "def generate_caption(model, tokenizer, features, max_length, img_id):\n",
        "    in_text = 'startseq'\n",
        "    for _ in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
        "        image_feature = features[img_id]\n",
        "        y_pred = model.predict([image_feature.reshape(1, 2048), sequence], verbose=0)\n",
        "        predicted_id = np.argmax(y_pred)\n",
        "        word = tokenizer.index_word.get(predicted_id)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    final_caption = in_text.replace('startseq', '').replace('endseq', '').strip()\n",
        "    return final_caption\n",
        "\n",
        "# Output file path\n",
        "output_file = '/content/drive/MyDrive/ML/image_caption/predicted_captions_1000.txt'\n",
        "generated_captions = {}\n",
        "\n",
        "# Get a random sample of 1000 image IDs that are also in features\n",
        "available_img_ids = list(set(descriptions.keys()) & set(features.keys()))\n",
        "sampled_img_ids = random.sample(available_img_ids, 1000)\n",
        "\n",
        "# Generate and save captions\n",
        "with open(output_file, 'w') as f:\n",
        "    for img_id in tqdm.tqdm(sampled_img_ids):\n",
        "        caption = generate_caption(model, tokenizer, features, max_sequence_length, img_id)\n",
        "        generated_captions[img_id] = caption\n",
        "        f.write(f\"{img_id}.jpg, {caption}\\n\")\n",
        "\n",
        "print(\"Predicted captions saved for 1000 images:\", output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mhq1RkdNCP1",
        "outputId": "087715f6-9665-482b-da81-6d7dedb86d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.43454095627453854\n",
            "BLEU-2: 0.2741517607012966\n",
            "BLEU-3: 0.1729977913888319\n",
            "BLEU-4: 0.11299715211521574\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "smooth = SmoothingFunction().method4\n",
        "bleu1, bleu2, bleu3, bleu4 = [], [], [], []\n",
        "\n",
        "for img_id in generated_captions:\n",
        "    reference_list = [caption.split() for caption in descriptions[img_id]]  # real captions\n",
        "    predicted = generated_captions[img_id].split()  # predicted caption\n",
        "\n",
        "    bleu1.append(sentence_bleu(reference_list, predicted, weights=(1, 0, 0, 0), smoothing_function=smooth))\n",
        "    bleu2.append(sentence_bleu(reference_list, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth))\n",
        "    bleu3.append(sentence_bleu(reference_list, predicted, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth))\n",
        "    bleu4.append(sentence_bleu(reference_list, predicted, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth))\n",
        "\n",
        "print(\"BLEU-1:\", np.mean(bleu1))\n",
        "print(\"BLEU-2:\", np.mean(bleu2))\n",
        "print(\"BLEU-3:\", np.mean(bleu3))\n",
        "print(\"BLEU-4:\", np.mean(bleu4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnRjxOU06TK1"
      },
      "source": [
        "### How to Interpret These BLEU Scores\n",
        "1. BLEU-1 (~0.43): This is unigram precision, meaning about 43% of the predicted words match the reference captions. This is pretty decent for Flickr8k and shows the model understands individual word predictions well.\n",
        "\n",
        "2. BLEU-2 to BLEU-4: These drop off because longer sequences (bigrams to 4-grams) are harder to match in image captioning. That’s expected. Your BLEU-4 of ~0.11 is within range for basic models on Flickr8k.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ax3sCSo6aon"
      },
      "source": [
        "### Model Architecture Context\n",
        "- ResNet50 encoder + LSTM decoder\n",
        "-  128 LSTM units\n",
        "-  Accuracy: 42% after 10 epochs on Flickr8k\n",
        "- That’s a solid baseline! But remember:\n",
        "- Accuracy isn't the best metric for this kind of sequence generation task (BLEU is better).\n",
        "- Still, 42% suggests it’s learning patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SogkwFFu54zH"
      },
      "source": [
        "### If you’re looking to improve performance:\n",
        "\n",
        "1. Train longer (20–30 epochs) – 10 might be too early.\n",
        "\n",
        "2. Increase LSTM units to 256 or 512 if RAM allows.\n",
        "\n",
        "3. Use Beam Search decoding instead of greedy — this improves BLEU scores.\n",
        "\n",
        "4. Add attention mechanism.\n",
        "\n",
        "5. More data — Flickr8k is small. Consider moving to Flickr30k or MS COCO later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7jbja5l5YEM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
